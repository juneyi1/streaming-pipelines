{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure HDFS 3 Sink Properties\n",
    "\n",
    "Let us configure HDFS 3 Sink Properties so that data can be pushed to HDFS from Kafka Topic.\n",
    "\n",
    "* We need to make sure that HDFS 3 Sink Connector plugin is already setup.\n",
    "\n",
    "```shell\n",
    "ls -ltr /opt/kafka/share/plugins/\n",
    "```\n",
    "\n",
    "* Make sure to create a working directory.\n",
    "\n",
    "```shell\n",
    "mkdir -p ~/kafka_connect/retail_logs_consume\n",
    "```\n",
    "\n",
    "* Define worker level properties as part of **retail_logs_standalone.properties**\n",
    "\n",
    "```shell\n",
    "bootstrap.servers=w01.itversity.com:9092,w02.itversity.com:9092\n",
    "\n",
    "key.converter=org.apache.kafka.connect.storage.StringConverter\n",
    "value.converter=org.apache.kafka.connect.storage.StringConverter\n",
    "\n",
    "key.converter.schemas.enable=true\n",
    "value.converter.schemas.enable=true\n",
    "\n",
    "offset.storage.file.filename=/home/itversity/kafka_connect/retail_logs_consume/retail.offsets\n",
    "# Flush much faster than normal, which is useful for testing/debugging\n",
    "offset.flush.interval.ms=10000\n",
    "\n",
    "plugin.path=/opt/kafka/share/plugins # This should point to the base directory of the plugins\n",
    "```\n",
    "\n",
    "* Define properties for sink. In my case the file name is **retail_logs_hdfs_sink.properties**.\n",
    "\n",
    "```shell\n",
    "name=hdfs-sink\n",
    "connector.class=io.confluent.connect.hdfs3.Hdfs3SinkConnector\n",
    "tasks.max=3\n",
    "confluent.topic.bootstrap.servers=w01.itversity.com:9092,w02.itversity.com:9092\n",
    "topics=dg_retail\n",
    "hdfs.url=hdfs://m01.itversity.com:9000/user/itversity/retail_consumer # Make sure to change this as per your HDFS folder.\n",
    "flush.size=1000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 36\n",
      "-rw-r--r-- 1 kafka kafka 32329 Apr 26 12:54 apache-curator-2.12.0.pom\n",
      "drwxr-xr-x 6 kafka kafka  4096 Apr 26 18:59 kafka-connect-hdfs\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr /opt/kafka/share/plugins/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/kafka_connect/retail_logs_consume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 84\n",
      "-rwxr-xr-x 1 itversity students   499 Apr 26 18:20 retail_logs_standalone.properties\n",
      "-rw-rw-r-- 1 itversity students 74589 Apr 26 18:28 kc.log\n",
      "-rwxr-xr-x 1 itversity students   309 Apr 26 18:33 retail_logs_hdfs_sink.properties\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr ~/kafka_connect/retail_logs_consume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrap.servers=w01.itversity.com:9092,w02.itversity.com:9092\n",
      "\n",
      "key.converter=org.apache.kafka.connect.storage.StringConverter\n",
      "value.converter=org.apache.kafka.connect.storage.StringConverter\n",
      "\n",
      "key.converter.schemas.enable=true\n",
      "value.converter.schemas.enable=true\n",
      "\n",
      "offset.storage.file.filename=/home/itversity/kafka_connect/retail_logs_consume/retail.offsets\n",
      "# Flush much faster than normal, which is useful for testing/debugging\n",
      "offset.flush.interval.ms=10000\n",
      "\n",
      "plugin.path=/opt/kafka/share/plugins\n"
     ]
    }
   ],
   "source": [
    "!cat /home/itversity/kafka_connect/retail_logs_consume/retail_logs_standalone.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name=hdfs-sink\n",
      "connector.class=io.confluent.connect.hdfs3.Hdfs3SinkConnector\n",
      "tasks.max=3\n",
      "confluent.topic.bootstrap.servers=w01.itversity.com:9092,w02.itversity.com:9092\n",
      "topics=dg_retail\n",
      "hdfs.url=hdfs://m01.itversity.com:9000/user/itversity/retail_consumer\n",
      "flush.size=1000\n",
      "plugin.path=/opt/kafka/share/plugins\n"
     ]
    }
   ],
   "source": [
    "!cat /home/itversity/kafka_connect/retail_logs_consume/retail_logs_hdfs_sink.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
