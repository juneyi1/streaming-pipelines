{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8d05356d-b391-4200-997f-8d2a06b270dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Add New GHActivity JSON files\n",
    "\n",
    "Let us add new files to the source and validate.\n",
    "* Create the function.\n",
    "* Run the function for all 24 hours for more days (2023-07-12, 2023-07-13, and 2023-07-14) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d76e11f6-0eed-4262-811f-4aa3e5df5a61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "import getpass\n",
    "\n",
    "def upload_gharchive_files_to_hdfs(file_name):\n",
    "    year = file_name[:4]\n",
    "    month = file_name[5:7]\n",
    "    dayofmonth = file_name[8:10]\n",
    "    username = getpass.getuser()\n",
    "    res = requests.get(f'https://data.gharchive.org/{file_name}')\n",
    "    target_local_folder = f'/home/{username}/data/ghactivity/year={year}/month={month}/dayofmonth={dayofmonth}'\n",
    "    subprocess.check_call(f'mkdir -p {target_local_folder}', shell=True)\n",
    "    target_file = open(f'{target_local_folder}/{file_name}', 'wb')\n",
    "    upload_res = target_file.write(res.content)\n",
    "    target_file.close()\n",
    "    target_hdfs_folder = f'/user/{username}/github/streaming/landing/ghactivity/year={year}/month={month}/dayofmonth={dayofmonth}'\n",
    "    subprocess.check_call(f'hdfs dfs -mkdir -p {target_hdfs_folder}', shell=True)\n",
    "    subprocess.check_call(f'hdfs dfs -put {target_local_folder}/{file_name} {target_hdfs_folder}', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 2023-07-12-0.json.gz\n",
      "Processing file 2023-07-12-1.json.gz\n",
      "Processing file 2023-07-12-2.json.gz\n",
      "Processing file 2023-07-12-3.json.gz\n",
      "Processing file 2023-07-12-4.json.gz\n",
      "Processing file 2023-07-12-5.json.gz\n",
      "Processing file 2023-07-12-6.json.gz\n",
      "Processing file 2023-07-12-7.json.gz\n",
      "Processing file 2023-07-12-8.json.gz\n",
      "Processing file 2023-07-12-9.json.gz\n",
      "Processing file 2023-07-12-10.json.gz\n",
      "Processing file 2023-07-12-11.json.gz\n",
      "Processing file 2023-07-12-12.json.gz\n",
      "Processing file 2023-07-12-13.json.gz\n",
      "Processing file 2023-07-12-14.json.gz\n",
      "Processing file 2023-07-12-15.json.gz\n",
      "Processing file 2023-07-12-16.json.gz\n",
      "Processing file 2023-07-12-17.json.gz\n",
      "Processing file 2023-07-12-18.json.gz\n",
      "Processing file 2023-07-12-19.json.gz\n",
      "Processing file 2023-07-12-20.json.gz\n",
      "Processing file 2023-07-12-21.json.gz\n",
      "Processing file 2023-07-12-22.json.gz\n",
      "Processing file 2023-07-12-23.json.gz\n"
     ]
    }
   ],
   "source": [
    "for hour in range(0, 24):\n",
    "    print(f'Processing file 2023-07-12-{hour}.json.gz')\n",
    "    upload_gharchive_files_to_hdfs(f'2023-07-12-{hour}.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ba80cf08-09f5-478d-ab0b-841ba1caee37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 2023-07-13-0.json.gz\n",
      "Processing file 2023-07-13-1.json.gz\n",
      "Processing file 2023-07-13-2.json.gz\n",
      "Processing file 2023-07-13-3.json.gz\n",
      "Processing file 2023-07-13-4.json.gz\n",
      "Processing file 2023-07-13-5.json.gz\n",
      "Processing file 2023-07-13-6.json.gz\n",
      "Processing file 2023-07-13-7.json.gz\n",
      "Processing file 2023-07-13-8.json.gz\n",
      "Processing file 2023-07-13-9.json.gz\n",
      "Processing file 2023-07-13-10.json.gz\n",
      "Processing file 2023-07-13-11.json.gz\n",
      "Processing file 2023-07-13-12.json.gz\n",
      "Processing file 2023-07-13-13.json.gz\n",
      "Processing file 2023-07-13-14.json.gz\n",
      "Processing file 2023-07-13-15.json.gz\n",
      "Processing file 2023-07-13-16.json.gz\n",
      "Processing file 2023-07-13-17.json.gz\n",
      "Processing file 2023-07-13-18.json.gz\n",
      "Processing file 2023-07-13-19.json.gz\n",
      "Processing file 2023-07-13-20.json.gz\n",
      "Processing file 2023-07-13-21.json.gz\n",
      "Processing file 2023-07-13-22.json.gz\n",
      "Processing file 2023-07-13-23.json.gz\n"
     ]
    }
   ],
   "source": [
    "for hour in range(0, 24):\n",
    "    print(f'Processing file 2023-07-13-{hour}.json.gz')\n",
    "    upload_gharchive_files_to_hdfs(f'2023-07-13-{hour}.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 2023-07-14-0.json.gz\n",
      "Processing file 2023-07-14-1.json.gz\n",
      "Processing file 2023-07-14-2.json.gz\n",
      "Processing file 2023-07-14-3.json.gz\n",
      "Processing file 2023-07-14-4.json.gz\n",
      "Processing file 2023-07-14-5.json.gz\n",
      "Processing file 2023-07-14-6.json.gz\n",
      "Processing file 2023-07-14-7.json.gz\n",
      "Processing file 2023-07-14-8.json.gz\n",
      "Processing file 2023-07-14-9.json.gz\n",
      "Processing file 2023-07-14-10.json.gz\n",
      "Processing file 2023-07-14-11.json.gz\n",
      "Processing file 2023-07-14-12.json.gz\n",
      "Processing file 2023-07-14-13.json.gz\n",
      "Processing file 2023-07-14-14.json.gz\n",
      "Processing file 2023-07-14-15.json.gz\n",
      "Processing file 2023-07-14-16.json.gz\n",
      "Processing file 2023-07-14-17.json.gz\n",
      "Processing file 2023-07-14-18.json.gz\n",
      "Processing file 2023-07-14-19.json.gz\n",
      "Processing file 2023-07-14-20.json.gz\n",
      "Processing file 2023-07-14-21.json.gz\n",
      "Processing file 2023-07-14-22.json.gz\n",
      "Processing file 2023-07-14-23.json.gz\n"
     ]
    }
   ],
   "source": [
    "for hour in range(0, 24):\n",
    "    print(f'Processing file 2023-07-14-{hour}.json.gz')\n",
    "    upload_gharchive_files_to_hdfs(f'2023-07-14-{hour}.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "09 Add New GHActivity JSON files",
   "notebookOrigID": 3881066461251993,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
