{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Output Modes\n",
    "\n",
    "Let us get an overview of output modes supported by Spark Structured Streaming. \n",
    "* **Append mode (default)** - This is the default mode, where only the new rows added to the Result Table since the last trigger will be outputted to the sink. This is supported for only those queries where rows added to the Result Table is never going to change. Hence, this mode guarantees that each row will be output only once (assuming fault-tolerant sink). For example, queries with only `select`, `where`, `map`, `flatMap`, `filter`, `join`, etc. will support Append mode.\n",
    "* **Complete mode** - The whole Result Table will be outputted to the sink after every trigger. This is supported for aggregation queries.\n",
    "* **Update mode** - (Available since Spark 2.1.1) Only the rows in the Result Table that were updated since the last trigger will be outputted to the sink. More information to be added in future releases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch Pyspark using below commands and run Spark Structured Streaming Code.\n",
    "\n",
    "**Using Pyspark2**\n",
    "\n",
    "```\n",
    "export PYSPARK_PYTHON=python3\n",
    "\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark3**\n",
    "\n",
    "```\n",
    "export PYSPARK_PYTHON=python3\n",
    "\n",
    "pyspark3 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example to read and print on console every 5 seconds. It will print data it have received since the last run every time. This is typically used to select, filter, etc. It will throw error if aggregations are used.\n",
    "\n",
    "```python\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '2')\n",
    "\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "log_messages = spark. \\\n",
    "    readStream. \\\n",
    "    format(\"socket\"). \\\n",
    "    option(\"host\", hostname). \\\n",
    "    option(\"port\", 9000). \\\n",
    "    load()\n",
    "\n",
    "log_messages. \\\n",
    "    writeStream. \\\n",
    "    outputMode(\"append\"). \\\n",
    "    format(\"console\"). \\\n",
    "    option('truncate', 'false'). \\\n",
    "    trigger(processingTime='5 seconds'). \\\n",
    "    start()\n",
    "```\n",
    "\n",
    "* Same as above. If we do not specify `outputMode`, then it will be **append** by default.\n",
    "\n",
    "```python\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '2')\n",
    "\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "log_messages = spark. \\\n",
    "    readStream. \\\n",
    "    format(\"socket\"). \\\n",
    "    option(\"host\", hostname). \\\n",
    "    option(\"port\", 9000). \\\n",
    "    load()\n",
    "\n",
    "log_messages. \\\n",
    "    writeStream. \\\n",
    "    format(\"console\"). \\\n",
    "    option('truncate', 'false'). \\\n",
    "    trigger(processingTime='5 seconds'). \\\n",
    "    start()\n",
    "```\n",
    "\n",
    "```python\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '2')\n",
    "\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "log_messages = spark. \\\n",
    "    readStream. \\\n",
    "    format(\"socket\"). \\\n",
    "    option(\"host\", hostname). \\\n",
    "    option(\"port\", 9000). \\\n",
    "    load()\n",
    "\n",
    "from pyspark.sql.functions import split, count, lit\n",
    "\n",
    "department_count = log_messages. \\\n",
    "    filter(split(split('value', ' ')[6], '/')[1] == 'department'). \\\n",
    "    select(split(split('value', ' ')[6], '/')[2].alias('department')). \\\n",
    "    groupBy('department'). \\\n",
    "    agg(count(lit(1)).alias('department_count'))\n",
    "\n",
    "department_count. \\\n",
    "    writeStream. \\\n",
    "    outputMode(\"append\"). \\\n",
    "    format(\"console\"). \\\n",
    "    option('truncate', 'false'). \\\n",
    "    trigger(processingTime='5 seconds'). \\\n",
    "    start()\n",
    "```\n",
    "\n",
    "```python\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "log_messages = spark. \\\n",
    "    readStream. \\\n",
    "    format(\"socket\"). \\\n",
    "    option(\"host\", hostname). \\\n",
    "    option(\"port\", 9000). \\\n",
    "    load()\n",
    "\n",
    "from pyspark.sql.functions import split, count, lit\n",
    "\n",
    "department_count = log_messages. \\\n",
    "    filter(split(split('value', ' ')[6], '/')[1] == 'department'). \\\n",
    "    select(split(split('value', ' ')[6], '/')[2].alias('department')). \\\n",
    "    groupBy('department'). \\\n",
    "    agg(count(lit(1)).alias('department_count'))\n",
    "\n",
    "department_count. \\\n",
    "    writeStream. \\\n",
    "    outputMode(\"complete\"). \\\n",
    "    format(\"console\"). \\\n",
    "    option('truncate', 'false'). \\\n",
    "    trigger(processingTime='5 seconds'). \\\n",
    "    start()\n",
    "```\n",
    "\n",
    "* Example for outputMode `update`\n",
    "\n",
    "```python\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "log_messages = spark. \\\n",
    "    readStream. \\\n",
    "    format(\"socket\"). \\\n",
    "    option(\"host\", hostname). \\\n",
    "    option(\"port\", 9000). \\\n",
    "    load()\n",
    "\n",
    "from pyspark.sql.functions import split, count, lit\n",
    "\n",
    "department_count = log_messages. \\\n",
    "    filter(split(split('value', ' ')[6], '/')[1] == 'department'). \\\n",
    "    select(split(split('value', ' ')[6], '/')[2].alias('department')). \\\n",
    "    groupBy('department'). \\\n",
    "    agg(count(lit(1)).alias('department_count'))\n",
    "\n",
    "department_count. \\\n",
    "    writeStream. \\\n",
    "    outputMode(\"update\"). \\\n",
    "    format(\"console\"). \\\n",
    "    option('truncate', 'false'). \\\n",
    "    trigger(processingTime='5 seconds'). \\\n",
    "    start()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
