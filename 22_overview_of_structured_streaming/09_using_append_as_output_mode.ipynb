{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using append as Output Mode\n",
    "\n",
    "Let us see few examples using `append` as output mode. For now, we will write to console with examples such as `filter`, `select`, etc.\n",
    "* If we try to use `append` with aggregations as part of transformations, it will fail with error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch Pyspark using below commands and run Spark Structured Streaming Code.\n",
    "\n",
    "**Using Pyspark2**\n",
    "\n",
    "```\n",
    "export PYSPARK_PYTHON=python3\n",
    "\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark3**\n",
    "\n",
    "```\n",
    "export PYSPARK_PYTHON=python3\n",
    "\n",
    "pyspark3 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example to read and print on console every 5 seconds. It will print data it have received since the last run every time. This is typically used to select, filter, etc. It will throw error if aggregations are used.\n",
    "\n",
    "```python\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '2')\n",
    "\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "log_messages = spark. \\\n",
    "    readStream. \\\n",
    "    format(\"socket\"). \\\n",
    "    option(\"host\", hostname). \\\n",
    "    option(\"port\", 9000). \\\n",
    "    load()\n",
    "\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "log_messages. \\\n",
    "    filter(split(split('value', ' ')[6], '/')[1] == 'department'). \\\n",
    "    writeStream. \\\n",
    "    outputMode(\"append\"). \\\n",
    "    format(\"console\"). \\\n",
    "    option('truncate', 'false'). \\\n",
    "    trigger(processingTime='5 seconds'). \\\n",
    "    start()\n",
    "```\n",
    "\n",
    "* Same as above. If we do not specify `outputMode`, then it will be **append** by default.\n",
    "\n",
    "```python\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '2')\n",
    "\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "log_messages = spark. \\\n",
    "    readStream. \\\n",
    "    format(\"socket\"). \\\n",
    "    option(\"host\", hostname). \\\n",
    "    option(\"port\", 9000). \\\n",
    "    load()\n",
    "\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "log_messages. \\\n",
    "    filter(split(split('value', ' ')[6], '/')[1] == 'department'). \\\n",
    "    writeStream. \\\n",
    "    format(\"console\"). \\\n",
    "    option('truncate', 'false'). \\\n",
    "    trigger(processingTime='5 seconds'). \\\n",
    "    start()\n",
    "```\n",
    "\n",
    "* This includes aggregations as part of the transformations. If we try to use `append` as output mode, it will fail.\n",
    "\n",
    "```python\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '2')\n",
    "\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "log_messages = spark. \\\n",
    "    readStream. \\\n",
    "    format(\"socket\"). \\\n",
    "    option(\"host\", hostname). \\\n",
    "    option(\"port\", 9000). \\\n",
    "    load()\n",
    "\n",
    "from pyspark.sql.functions import split, count, lit\n",
    "\n",
    "department_count = log_messages. \\\n",
    "    filter(split(split('value', ' ')[6], '/')[1] == 'department'). \\\n",
    "    select(split(split('value', ' ')[6], '/')[2].alias('department')). \\\n",
    "    groupBy('department'). \\\n",
    "    agg(count(lit(1)).alias('department_count'))\n",
    "\n",
    "department_count. \\\n",
    "    writeStream. \\\n",
    "    outputMode(\"append\"). \\\n",
    "    format(\"console\"). \\\n",
    "    option('truncate', 'false'). \\\n",
    "    trigger(processingTime='5 seconds'). \\\n",
    "    start()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
