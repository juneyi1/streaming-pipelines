{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3b8fce",
   "metadata": {},
   "source": [
    "## Writing Streaming Data to Files\n",
    "\n",
    "As we have successfully read the data and see it is being processed using `writeStream.format('console')`, now it is time for us to understand how the data can be written to files.\n",
    "\n",
    "Here are the steps we need to follow to write the data to files:\n",
    "1. Ensure the logs are being redirected to Netcat Webserver\n",
    "2. Read the data using `spark.readStream` with `format('socket')`\n",
    "3. Use `writeStream.format` with appropriate options related to the file format. We will be using `writeStream.format('csv')` and hence we need to specify checkpoint and target location.\n",
    "\n",
    "```python\n",
    "socketDF. \\\n",
    "    writeStream. \\\n",
    "    format(\"csv\"). \\\n",
    "    option(\"checkpointLocation\", \"/user/itversity/retail_logs/gen_logs/checkpoint\"). \\\n",
    "    option(\"path\", \"/user/itversity/retail_logs/gen_logs/data\"). \\\n",
    "    start()\n",
    "```\n",
    "\n",
    "4. Validate both the checkpoint location as well as data location in which files are being copied to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42a3efe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Overview of Structured Streaming'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24ea56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "hostname = socket.gethostname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f14baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_messages = spark. \\\n",
    "    readStream. \\\n",
    "    format('socket'). \\\n",
    "    option('host', hostname). \\\n",
    "    option('port', 9000). \\\n",
    "    load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e637716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1869d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'checkpointLocation must be specified either through option(\"checkpointLocation\", ...) or SparkSession.conf.set(\"spark.sql.streaming.checkpointLocation\", ...);'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o91.start.\n: org.apache.spark.sql.AnalysisException: checkpointLocation must be specified either through option(\"checkpointLocation\", ...) or SparkSession.conf.set(\"spark.sql.streaming.checkpointLocation\", ...);\n\tat org.apache.spark.sql.streaming.StreamingQueryManager$$anonfun$4.apply(StreamingQueryManager.scala:234)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager$$anonfun$4.apply(StreamingQueryManager.scala:229)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:228)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:322)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:325)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b204350d8033>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mwriteStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'/user/{username}/retail_logs/gen_logs/data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'checkpointLocation must be specified either through option(\"checkpointLocation\", ...) or SparkSession.conf.set(\"spark.sql.streaming.checkpointLocation\", ...);'"
     ]
    }
   ],
   "source": [
    "log_messages. \\\n",
    "    writeStream. \\\n",
    "    format('csv'). \\\n",
    "    option('path', f'/user/{username}/retail_logs/gen_logs/data'). \\\n",
    "    start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b40f787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f81bd057e10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_messages. \\\n",
    "    writeStream. \\\n",
    "    format('csv'). \\\n",
    "    option(\"checkpointLocation\", f'/user/{username}/retail_logs/gen_logs/checkpoint'). \\\n",
    "    option('path', f'/user/{username}/retail_logs/gen_logs/data'). \\\n",
    "    trigger(processingTime='5 seconds'). \\\n",
    "    start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfb6a0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 items\n",
      "drwxr-xr-x   - itversity itversity          0 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/_spark_metadata\n",
      "-rw-r--r--   3 itversity itversity        633 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00000-16713de8-c877-45d0-818c-78d9333b0d11-c000.csv\n",
      "-rw-r--r--   3 itversity itversity          0 2021-08-22 09:56 /user/itversity/retail_logs/gen_logs/data/part-00000-3e52a404-1f53-43f9-97a7-0db2a90c6ff9-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        680 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00000-5d1d48a4-57dd-4ede-94ba-c7ebd71979e1-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        653 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00000-640f67e4-2c52-4bdf-ba08-5ba69932f277-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        623 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00000-68d61ad5-c1aa-46da-9ab2-4d1a3b10ce68-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        622 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00000-7428eeec-4530-4c76-a1ce-5a86bb78fd8e-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        687 2021-08-22 09:56 /user/itversity/retail_logs/gen_logs/data/part-00000-7f0093e5-cd73-4edb-a1a8-a99ca763f1b0-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        685 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00000-885bac72-f880-4a73-aa93-c4765c90590e-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        635 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00000-91637424-25d7-4475-81e5-ce56e741fe5a-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        642 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00000-9c2e77d6-ea71-4ba4-adc4-48e6b5651c9f-c000.csv\n",
      "-rw-r--r--   3 itversity itversity      48333 2021-08-22 09:56 /user/itversity/retail_logs/gen_logs/data/part-00000-af1fb000-d6da-4136-b846-77a6b85c46e3-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        658 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00000-c8dcdc70-40ee-4378-8052-401cb9d2c0f6-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        620 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00000-e3762293-f16f-4ace-ab9e-e840f4447bb3-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        594 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00000-e4b0c1de-3362-4ce5-8522-ff7b967d4876-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        631 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00000-e64b91d2-6239-4a18-8553-b9880909fe2e-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        436 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00001-037d9a48-f363-4802-aa74-aedcfd619461-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        419 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00001-05ce9f78-91f8-4d9e-bbd3-07370756b710-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        417 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00001-122f4e1d-722e-49e1-9833-f1b768422c77-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        443 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00001-175a9c64-d97a-40fc-81af-cc54d649d878-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        416 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00001-5aee8619-f587-4ef3-af90-ec6ffafdee25-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        446 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00001-8147f2e8-39ec-4b73-8718-9bd85091db96-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        368 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00001-8bf95a90-8886-4deb-9855-c4ab50b849f5-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        396 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00001-967cd26a-bf4e-44bd-b0e6-66bca2cda5dc-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        461 2021-08-22 09:56 /user/itversity/retail_logs/gen_logs/data/part-00001-9b0c65b5-f373-4d16-abc3-a02317ffbace-c000.csv\n",
      "-rw-r--r--   3 itversity itversity      47462 2021-08-22 09:56 /user/itversity/retail_logs/gen_logs/data/part-00001-ac620d6e-7c3c-4414-a2c7-442f536e2bc6-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        457 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00001-b89560fe-8dcb-46eb-a2f0-0dc8a8590c42-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        444 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00001-c14f594e-72f7-4b44-b6cb-0f014a051625-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        421 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00001-da7e91fa-7622-4dbc-9c76-127e30198895-c000.csv\n",
      "-rw-r--r--   3 itversity itversity        457 2021-08-22 09:57 /user/itversity/retail_logs/gen_logs/data/part-00001-e35cdafe-6556-4c11-a63a-cb4603e8efb5-c000.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/${USER}/retail_logs/gen_logs/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0bc201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64555ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
