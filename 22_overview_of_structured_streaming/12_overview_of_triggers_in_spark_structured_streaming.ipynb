{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9cf219",
   "metadata": {},
   "source": [
    "## Overview of Triggers in Spark Structured Streaming\n",
    "\n",
    "Let us get an overview of triggers supported by Spark Structured Streaming.\n",
    "\n",
    "The trigger settings of a streaming query define the timing of streaming data processing, whether the query is going to be executed as micro-batch query with a fixed batch interval or as a continuous processing query. Here are the different kinds of triggers that are supported.\n",
    "\n",
    "Here are the different trigger types:\n",
    "* Unspecified (Default)\n",
    "* Fixed interval micro-batches\n",
    "* One-time micro-batch\n",
    "* Continuous with fixed checkpoint interval (experimental)\n",
    "\n",
    "**Unspecified (Default)**\n",
    "\n",
    "If no trigger setting is explicitly specified, then by default, the query will be executed in micro-batch mode, where micro-batches will be generated as soon as the previous micro-batch has completed processing.\n",
    "\n",
    "**Fixed interval micro-batches**\n",
    "\n",
    "The query will be executed with micro-batches mode, where micro-batches will be kicked off at the user-specified intervals.\n",
    "* If the previous micro-batch completes within the interval, then the engine will wait until the interval is over before kicking off the next micro-batch.\n",
    "* If the previous micro-batch takes longer than the interval to complete (i.e. if an interval boundary is missed), then the next micro-batch will start as soon as the previous one completes (i.e., it will not wait for the next interval boundary).\n",
    "* If no new data is available, then no micro-batch will be kicked off.\n",
    "\n",
    "**One-time micro-batch**\n",
    "\n",
    "The query will execute **only one** micro-batch to process all the available data and then stop on its own. This is useful in scenarios you want to periodically spin up a cluster, process everything that is available since the last period, and then shutdown the cluster. In some case, this may lead to significant cost savings.\n",
    "\n",
    "**Continuous with fixed checkpoint interval (experimental)**\n",
    "\n",
    "The query will be executed in the new low-latency, continuous processing mode. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a71815",
   "metadata": {},
   "source": [
    "Launch Pyspark using below commands and run Spark Structured Streaming Code.\n",
    "\n",
    "**Using Pyspark2**\n",
    "\n",
    "```\n",
    "export PYSPARK_PYTHON=python3\n",
    "\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark3**\n",
    "\n",
    "```\n",
    "export PYSPARK_PYTHON=python3\n",
    "\n",
    "pyspark3 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43670351",
   "metadata": {},
   "source": [
    "So far we have seen examples using **Fixed interval micro-batches**. We will see other examples in subsequent sections.\n",
    "\n",
    "```python\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '2')\n",
    "\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "log_messages = spark. \\\n",
    "    readStream. \\\n",
    "    format(\"socket\"). \\\n",
    "    option(\"host\", hostname). \\\n",
    "    option(\"port\", 9000). \\\n",
    "    load()\n",
    "\n",
    "log_messages. \\\n",
    "    writeStream. \\\n",
    "    outputMode(\"append\"). \\\n",
    "    format(\"console\"). \\\n",
    "    option('truncate', 'false'). \\\n",
    "    trigger(processingTime='5 seconds'). \\\n",
    "    start()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d90f7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
